import os
import shutil
import sys

import torch
import torch.nn
import torch.quantization
import numpy as np
import collections
import json
import onnx
import onnx_tf
import tensorflow as tf
sys.path.append('./include')
import subprocess

#  generated by pics.
#  we need to set dependency on cmakelist
import flatbuffers
from include.circle.Model import Model
from include.circle.SubGraph import SubGraph


class TorchExtractor:
    qdtype_mapping = {
        torch.quint8: {'str': "uint8", 'np': np.uint8},
        torch.qint8: {'str': "int8", 'np': np.int8},
        torch.qint32: {'str': "int32", 'np': np.int32}
    }

    @staticmethod
    def permute(tensor: torch.Tensor) -> torch.Tensor:
        dim = len(tensor.shape)
        if dim == 4:  # NCHW to NHWC
            tensor = tensor.permute(0, 2, 3, 1)
        return tensor
    
    @staticmethod
    def quantize_bias(tensor, scale, zero_point, dtype=np.int8):
        if dtype not in (np.uint8, np.int8, np.int32):
            raise Exception('Check dtype of bias quantization')
        bias = tensor.clone().detach().numpy()
        bias = bias / scale + zero_point
        return bias.astype(dtype)


    def __init__(self, dir_path: str, json_path: str, json_file_name: str):
        self.__np_idx = 0
        self.__dir_path = dir_path
        self.__json_path = json_path
        self.__json_file_name = json_file_name


    def extract_module(self, module: torch.nn.Module) -> collections.OrderedDict:
        tree = collections.OrderedDict()
        for name, tensor in module.state_dict().items():
            print(name, tensor)
            layer = name[:name.rfind(".")]
            if layer in tree:
                data = tree[layer]
            else:
                data = {}
                tree[layer] = data
            tensor_name = name[name.rfind(".") + 1:]
            data[tensor_name] = self.permute(tensor)
        return tree
    
    
    def __save_np(self, data):
        file_name = str(self.__np_idx) + ".npy"
        if data.shape==():
            data = np.array([data])
        #  python interpreter 64 may be uses float64 for default
        if data.dtype == np.dtype(np.float64):
            data = data.astype(np.float32)
        np.save(os.path.join(self.__dir_path, file_name), data)
        self.__np_idx += 1
        return file_name
    

    def __from_tensor(self, tensor):
        if tensor is None:
            raise Exception('tensor is null')
        data = {}
        if tensor.qscheme() in (torch.per_tensor_affine, torch.per_tensor_symmetric):
            data['scale'] = self.__save_np(np.array(tensor.q_scale()))
            data['zerop'] = self.__save_np(np.array(tensor.q_zero_point()))
            data['quantized_dimension'] = 0
        elif tensor.qscheme() in (torch.per_channel_affine, torch.per_channel_symmetric, torch.per_channel_affine_float_qparams):
            data['scale'] = self.__save_np(tensor.q_per_channel_scales().numpy())
            data['zerop'] = self.__save_np(tensor.q_per_channel_zero_points().numpy())
            data['quantized_dimension'] = tensor.q_per_channel_axis()

        # https://pytorch.org/docs/stable/quantization.html#quantized-tensor
        # https://pytorch.org/docs/1.7.0/quantization.html#quantized-tensors
        # According to documentation, pytorch latest tensor support quint8, qint8, qint32, float16
        # But we use pytorch 1.7.0 due to onnx-tf, pytorch 1.7.0 do not support float16
        if tensor.dtype == torch.qint8:
            data['value'] = self.__save_np(torch.int_repr(tensor).numpy())
        else:
            data['value'] = self.__save_np(tensor.numpy())
        data['dtype'] = self.qdtype_mapping[tensor.dtype]['str']
        return data
    

    def generate_files(self, tree: collections.OrderedDict, mapping: dict):
        mapped_data = {}
        not_mapped_data = {}
        if not os.path.exists(self.__dir_path):
            os.makedirs(self.__dir_path, exist_ok=True)

        for name, layer in tree.items():
            default_dtype = 'uint8'
            if "weight" in layer:
                w_name = name + '.weight'
                tensor = layer['weight']
                if w_name in mapping:
                    data = mapped_data
                    w_name = mapping[w_name]
                else:
                    data = not_mapped_data
                if tensor.is_quantized:
                    default_dtype = self.qdtype_mapping[tensor.dtype]['str']
                    data[w_name] = self.__from_tensor(tensor=tensor)
            if "scale" in layer and "zero_point" in layer:
                # not sure about torch operator's scale and zero_point is for bias or input
                # let's assume operator's scale and zero_point is used for both bias and input
                scale = layer['scale'].numpy()
                zero_point = layer['zero_point'].numpy()

                layer_name = name
                if layer_name in mapping:
                    layer_name = mapping[layer_name]
                    data = mapped_data
                else:
                    data = not_mapped_data

                s_np = self.__save_np(scale)
                z_np = self.__save_np(zero_point)
                # data[layer_name] = {
                #     'scale': s_np,
                #     'zerop': z_np,
                #     'dtype': default_dtype,
                #     'quantized_dimension': 0
                # }

                b_name = name + '.bias'
                if b_name in mapping:
                    b_name = mapping[b_name]
                    data = mapped_data
                else:
                    data = not_mapped_data

                if "bias" in layer:
                    quantized_bias = self.quantize_bias(layer['bias'], scale, zero_point)
                    data[b_name] = {
                        'scale': s_np,
                        'zerop': z_np,
                        'dtype': default_dtype,
                        'value': self.__save_np(quantized_bias),
                        'quantized_dimension': 0
                    }
        with open(self.__json_path, 'w') as json_file:
            json.dump(mapped_data, json_file)
        with open(os.path.join(self.__dir_path, 'not_mapped_' + self.__json_file_name), 'w') as json_file:
            json.dump(not_mapped_data, json_file)